{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1mK43DuNULS"
      },
      "source": [
        "# Colab training routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRml9OKENULT"
      },
      "source": [
        "## Set Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWEp8qmzNULT",
        "outputId": "090876d4-7383-418f-fb4b-e8b38602c4d8"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/LeonardoDiCaterina/DL.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3E5oSQYNULT"
      },
      "source": [
        "## Hardware Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAuB4NLsNULT",
        "outputId": "d294ca40-76af-45bb-95f0-fbfb53ffbb21"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jZjPW7UNULT",
        "outputId": "1f7e1f07-439e-42f6-b0dd-9192e1be62d1"
      },
      "outputs": [],
      "source": [
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jJMItd4NULU",
        "outputId": "66e861f9-932d-4c2c-fe8c-5da7ae51a3f7"
      },
      "outputs": [],
      "source": [
        "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
        "print(\"Is GPU available:\", tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHV0oRw4NULU",
        "outputId": "c8a43bd3-3a97-449a-e808-0b93571a33ba"
      },
      "outputs": [],
      "source": [
        "dummy_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(256, 256, 3)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "dummy_input = tf.random.normal((1, 256, 256, 3))\n",
        "out = dummy_model(dummy_input)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHEaBOrRNULU"
      },
      "source": [
        "## prepare the direcory for the dataset and preprcess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PS91nSAtNULU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: /content/DL/data: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! mkdir /content/DL/data/downloaded_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMalorrWNULU",
        "outputId": "56de2deb-749b-469b-d578-18de5f44a153"
      },
      "outputs": [],
      "source": [
        "! gdown --id 1PyxqW_nsORX4PetkQo6OIL0mUL1pFsTD --output /content/DL/data/downloaded_dataset/rare_species.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7kKJ4nMNULU",
        "outputId": "735fe0cf-13a0-449c-d265-43025e0a728d"
      },
      "outputs": [],
      "source": [
        "! unzip data/downloaded_dataset/rare_species.zip -d data/downloaded_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kqQYrCUPHit",
        "outputId": "57033093-a1d9-4650-cec3-b7fb0de533fb"
      },
      "outputs": [],
      "source": [
        "%cd DL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TRpnF68NULU"
      },
      "source": [
        "copy this in the config file to the `preprocessing_config.py` file\n",
        "```python\n",
        "DATA_DIR = 'data/downloaded_dataset'\n",
        "DEST_DIR = 'data/rearranged'\n",
        "CSV_PATH = f'{DATA_DIR}/metadata.csv'\n",
        "N_SPLITS = 5 #it's a positive integer\n",
        "TEST_SIZE = 0.2 # it's a ratio therefore has to be between 0 and 1\n",
        "OVERSAMPLE = True\n",
        "LOG_LEVEL = 'INFO'\n",
        "LABEL_COL = 'family'\n",
        "OVERCLASS_COL = None\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnGuenjWQsR7",
        "outputId": "fc5237a3-f196-416c-f14f-cd42c7f76789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-02 16:20:36,948 - __main__ - INFO - Removing existing directory data/rearranged\n",
            "2025-05-02 16:20:36,948 - __main__ - INFO - Starting preprocessing pipeline\n",
            "2025-05-02 16:20:37,002 - __main__ - INFO - Loaded metadata with 11983 entries\n",
            "2025-05-02 16:20:37,002 - data_preprocessing.splitting - INFO - Splitting dataset into train/test and folds\n",
            "2025-05-02 16:20:37,030 - data_preprocessing.splitting - INFO - Created 5 stratified folds\n",
            "2025-05-02 16:20:37,030 - __main__ - INFO - Data splitting completed\n",
            "2025-05-02 16:20:37,030 - __main__ - INFO - Creating directory data/rearranged/fold_0\n",
            "2025-05-02 16:20:37,030 - __main__ - INFO - Renaming and saving the fold 0\n",
            "2025-05-02 16:20:37,030 - data_preprocessing.augmentation - INFO - Starting oversampling and saving process\n",
            "2025-05-02 16:20:37,030 - data_preprocessing.augmentation - INFO - origin_root: data/downloaded_dataset --> dest_root: data/rearranged/fold_0\n",
            "2025-05-02 16:20:37,030 - data_preprocessing.data_utils - INFO - Calculating number of oversampling copies per class\n",
            "2025-05-02 16:20:37,030 - data_preprocessing.data_utils - INFO - Analyzing class proportions\n",
            "100%|██████████████████████████████████████| 1918/1918 [00:02<00:00, 776.09it/s]\n",
            "2025-05-02 16:20:39,545 - data_preprocessing.augmentation - INFO - Oversampling and saving process completed\n",
            "2025-05-02 16:20:39,555 - __main__ - INFO - Saved fold metadata to data/rearranged/fold_0/metadata_00.csv\n",
            "2025-05-02 16:20:39,555 - __main__ - INFO - Saved fold 0 to data/rearranged/fold_0\n",
            "2025-05-02 16:20:39,555 - __main__ - INFO - Creating directory data/rearranged/fold_1\n",
            "2025-05-02 16:20:39,555 - __main__ - INFO - Renaming and saving the fold 1\n",
            "2025-05-02 16:20:39,555 - data_preprocessing.augmentation - INFO - Starting oversampling and saving process\n",
            "2025-05-02 16:20:39,555 - data_preprocessing.augmentation - INFO - origin_root: data/downloaded_dataset --> dest_root: data/rearranged/fold_1\n",
            "2025-05-02 16:20:39,555 - data_preprocessing.data_utils - INFO - Calculating number of oversampling copies per class\n",
            "2025-05-02 16:20:39,555 - data_preprocessing.data_utils - INFO - Analyzing class proportions\n",
            "100%|██████████████████████████████████████| 1917/1917 [00:02<00:00, 883.41it/s]\n",
            "2025-05-02 16:20:41,729 - data_preprocessing.augmentation - INFO - Oversampling and saving process completed\n",
            "2025-05-02 16:20:41,735 - __main__ - INFO - Saved fold metadata to data/rearranged/fold_1/metadata_01.csv\n",
            "2025-05-02 16:20:41,735 - __main__ - INFO - Saved fold 1 to data/rearranged/fold_1\n",
            "2025-05-02 16:20:41,735 - __main__ - INFO - Creating directory data/rearranged/fold_2\n",
            "2025-05-02 16:20:41,735 - __main__ - INFO - Renaming and saving the fold 2\n",
            "2025-05-02 16:20:41,735 - data_preprocessing.augmentation - INFO - Starting oversampling and saving process\n",
            "2025-05-02 16:20:41,735 - data_preprocessing.augmentation - INFO - origin_root: data/downloaded_dataset --> dest_root: data/rearranged/fold_2\n",
            "2025-05-02 16:20:41,736 - data_preprocessing.data_utils - INFO - Calculating number of oversampling copies per class\n",
            "2025-05-02 16:20:41,736 - data_preprocessing.data_utils - INFO - Analyzing class proportions\n",
            "100%|█████████████████████████████████████| 1917/1917 [00:01<00:00, 1036.59it/s]\n",
            "2025-05-02 16:20:43,589 - data_preprocessing.augmentation - INFO - Oversampling and saving process completed\n",
            "2025-05-02 16:20:43,593 - __main__ - INFO - Saved fold metadata to data/rearranged/fold_2/metadata_02.csv\n",
            "2025-05-02 16:20:43,593 - __main__ - INFO - Saved fold 2 to data/rearranged/fold_2\n",
            "2025-05-02 16:20:43,593 - __main__ - INFO - Creating directory data/rearranged/fold_3\n",
            "2025-05-02 16:20:43,593 - __main__ - INFO - Renaming and saving the fold 3\n",
            "2025-05-02 16:20:43,593 - data_preprocessing.augmentation - INFO - Starting oversampling and saving process\n",
            "2025-05-02 16:20:43,593 - data_preprocessing.augmentation - INFO - origin_root: data/downloaded_dataset --> dest_root: data/rearranged/fold_3\n",
            "2025-05-02 16:20:43,593 - data_preprocessing.data_utils - INFO - Calculating number of oversampling copies per class\n",
            "2025-05-02 16:20:43,593 - data_preprocessing.data_utils - INFO - Analyzing class proportions\n",
            "100%|█████████████████████████████████████| 1917/1917 [00:01<00:00, 1108.00it/s]\n",
            "2025-05-02 16:20:45,326 - data_preprocessing.augmentation - INFO - Oversampling and saving process completed\n",
            "2025-05-02 16:20:45,333 - __main__ - INFO - Saved fold metadata to data/rearranged/fold_3/metadata_03.csv\n",
            "2025-05-02 16:20:45,333 - __main__ - INFO - Saved fold 3 to data/rearranged/fold_3\n",
            "2025-05-02 16:20:45,333 - __main__ - INFO - Creating directory data/rearranged/fold_4\n",
            "2025-05-02 16:20:45,333 - __main__ - INFO - Renaming and saving the fold 4\n",
            "2025-05-02 16:20:45,333 - data_preprocessing.augmentation - INFO - Starting oversampling and saving process\n",
            "2025-05-02 16:20:45,333 - data_preprocessing.augmentation - INFO - origin_root: data/downloaded_dataset --> dest_root: data/rearranged/fold_4\n",
            "2025-05-02 16:20:45,334 - data_preprocessing.data_utils - INFO - Calculating number of oversampling copies per class\n",
            "2025-05-02 16:20:45,334 - data_preprocessing.data_utils - INFO - Analyzing class proportions\n",
            "100%|█████████████████████████████████████| 1917/1917 [00:01<00:00, 1117.42it/s]\n",
            "2025-05-02 16:20:47,055 - data_preprocessing.augmentation - INFO - Oversampling and saving process completed\n",
            "2025-05-02 16:20:47,058 - __main__ - INFO - Saved fold metadata to data/rearranged/fold_4/metadata_04.csv\n",
            "2025-05-02 16:20:47,059 - __main__ - INFO - Saved fold 4 to data/rearranged/fold_4\n",
            "2025-05-02 16:20:47,059 - __main__ - INFO - Creating directory data/rearranged/test\n",
            "2025-05-02 16:20:47,059 - __main__ - INFO - Renaming and saving test set\n",
            "2025-05-02 16:20:47,059 - data_preprocessing.augmentation - INFO - Starting oversampling and saving process\n",
            "2025-05-02 16:20:47,059 - data_preprocessing.augmentation - INFO - origin_root: data/downloaded_dataset --> dest_root: data/rearranged/test\n",
            "2025-05-02 16:20:47,059 - data_preprocessing.data_utils - INFO - Calculating number of oversampling copies per class\n",
            "2025-05-02 16:20:47,059 - data_preprocessing.data_utils - INFO - Analyzing class proportions\n",
            "100%|█████████████████████████████████████| 2397/2397 [00:02<00:00, 1094.20it/s]\n",
            "2025-05-02 16:20:49,260 - data_preprocessing.augmentation - INFO - Oversampling and saving process completed\n",
            "2025-05-02 16:20:49,272 - __main__ - INFO - Saved test metadata to data/rearranged/test/metadata_test.csv\n",
            "2025-05-02 16:20:49,272 - __main__ - INFO - Saved test set to data/rearranged/test\n",
            "2025-05-02 16:20:49,272 - __main__ - INFO - Preprocessing pipeline completed\n"
          ]
        }
      ],
      "source": [
        "! python -m data_preprocessing.main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLubRfO1lwyn"
      },
      "source": [
        "# train the model with the whole data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "copy this in the config file to the `training/training_config.py` file\n",
        "```python\n",
        "NUM_CLASSES = 202\n",
        "INPUT_SHAPE = (256, 256, 3)\n",
        "N_EPOCHS_4CV = 5 #number of epochs for cross-validation\n",
        "N_EPOCHS_4FULL_TRAIN = 10 #number of epochs for test set\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-02 16:23:26,111 - data_preprocessing.data_loading - INFO - Loading data from data/rearranged\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2397 files belonging to 202 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-02 16:23:26,608 - data_preprocessing.data_loading - INFO - Loaded test dataset with 75 batches\n",
            "2025-05-02 16:23:26,608 - data_preprocessing.data_loading - INFO - Test dataset shape: (TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 202), dtype=tf.float32, name=None))\n",
            "Loading folds:   0%|          | 0/5 [00:00<?, ?it/s]2025-05-02 16:23:26,610 - data_preprocessing.data_loading - INFO - Loading fold 0 from data/rearranged/fold_0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1918 files belonging to 202 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-02 16:23:26,709 - data_preprocessing.data_loading - INFO - Loading fold 1 from data/rearranged/fold_1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1917 files belonging to 202 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading folds:  40%|████      | 2/5 [00:00<00:00,  6.87it/s]2025-05-02 16:23:26,903 - data_preprocessing.data_loading - INFO - Loading fold 2 from data/rearranged/fold_2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1917 files belonging to 202 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-02 16:23:26,996 - data_preprocessing.data_loading - INFO - Loading fold 3 from data/rearranged/fold_3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1917 files belonging to 202 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading folds:  80%|████████  | 4/5 [00:00<00:00,  7.13it/s]2025-05-02 16:23:27,175 - data_preprocessing.data_loading - INFO - Loading fold 4 from data/rearranged/fold_4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1917 files belonging to 202 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading folds: 100%|██████████| 5/5 [00:00<00:00,  7.57it/s]\n",
            "2025-05-02 16:23:27,271 - data_preprocessing.data_loading - INFO - Loaded 5 folds\n"
          ]
        }
      ],
      "source": [
        "# load the dataset\n",
        "from data_preprocessing.data_loading import load_data\n",
        "train_folds,test_ds = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-FShpzgl0jW"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "from training.model_selection_utils import build_model\n",
        "\n",
        "from tf.keras.optimizers import Adam\n",
        "from tf.keras.callbacks import EarlyStopping\n",
        "\n",
        "def train_best_model(train_folds, test_data,configuration, input_shape = (256, 256, 3) , num_classes = 202 ,\n",
        "                      epochs=10):\n",
        "\n",
        "    model_name, freeze_until, dense_layers, learning_rate = configuration\n",
        "\n",
        "    # Combine all folds into one dataset\n",
        "    full_train_ds = reduce(lambda x, y: x.concatenate(y), train_folds)\n",
        "    full_train_ds = full_train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Build model with best configuration\n",
        "    model = build_model(model_name, freeze_until, dense_layers, input_shape, num_classes)\n",
        "\n",
        "    # Compile with specified learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Optional early stopping for safety\n",
        "    early_stop = EarlyStopping(monitor='accuracy', patience=3, restore_best_weights=True)\n",
        "\n",
        "    print(f\"\\nTraining final model with freeze_until={freeze_until}, \"\n",
        "          f\"dense_layers={dense_layers}, learning_rate={learning_rate:.2e}\")\n",
        "\n",
        "    # Fit the model with validation data\n",
        "    history = model.fit(full_train_ds,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=test_data,\n",
        "                        callbacks=[early_stop],\n",
        "                        verbose=1)\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "o-MhSQfcmDIC",
        "outputId": "59d621f1-82ac-4ad1-8cb2-5b2b62d52ac8"
      },
      "outputs": [],
      "source": [
        "configuration = ('ResNet50', 50, [512], 0.0001)\n",
        "best_model, best_history = train_best_model(train_folds,test_ds,configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        "    \"\"\"\n",
        "    Plots training and validation accuracy and loss from a Keras history object.\n",
        "\n",
        "    Args:\n",
        "        history: A History object returned by model.fit().\n",
        "    \"\"\"\n",
        "    acc = history.history.get('accuracy')\n",
        "    val_acc = history.history.get('val_accuracy')\n",
        "    loss = history.history.get('loss')\n",
        "    val_loss = history.history.get('val_loss')\n",
        "\n",
        "    if acc and loss:\n",
        "        epochs = range(1, len(acc) + 1)\n",
        "\n",
        "        # Plot accuracy\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, acc, 'o-', label='Training Accuracy')\n",
        "        if val_acc:\n",
        "            plt.plot(epochs, val_acc, 's-', label='Validation Accuracy')\n",
        "        plt.title('Model Accuracy per Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot loss\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, loss, 'o-', label='Training Loss')\n",
        "        if val_loss:\n",
        "            plt.plot(epochs, val_loss, 's-', label='Validation Loss')\n",
        "        plt.title('Model Loss per Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"History object does not contain accuracy or loss data.\")\n",
        "        \n",
        "plot_history(best_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tqdm\n",
        "\n",
        "def plot_confusion_matrix_from_dataset_with_class_names(model, test_dataset):\n",
        "    \"\"\"\n",
        "    Generates and plots a confusion matrix using class names from the dataset.\n",
        "\n",
        "    Args:\n",
        "        model: A trained TensorFlow Keras model.\n",
        "        test_dataset: A TensorFlow PrefetchDataset with a 'class_names' attribute.\n",
        "    \"\"\"\n",
        "    if not hasattr(test_dataset, 'class_names'):\n",
        "        raise AttributeError(\"The test_dataset must have a 'class_names' attribute.\")\n",
        "\n",
        "    #class_names = test_dataset.class_names\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    print(len(test_dataset))\n",
        "    for images, labels in tqdm(test_dataset, desc=\"Predicting\"):\n",
        "        y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "        y_pred.extend(np.argmax(model.predict(images, verbose=0), axis=1))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    normalized_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(normalized_cm, annot=False, fmt='.2f', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Normalized Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix_from_dataset_with_class_names(best_model, test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DL_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
