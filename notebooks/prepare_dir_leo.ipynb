{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "555b56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5c542",
   "metadata": {},
   "source": [
    "1. Function to split metadata into train, validation and test sets [DONE]\n",
    "2. Fucntion to write files from a split into required directury hierarchy [PENDING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9950394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def create_split(df: pd.DataFrame, n_folds: int, test_ratio: float) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame with two columns (file path, target class) into stratified train, validation, and test sets.\n",
    "\n",
    "    :param df (pd.DataFrame): Input DataFrame with [file_path, target_class] columns\n",
    "    :param val_ratio (float): Proportion of data for validation\n",
    "    :param test_ratio (float): Proportion of data for testing \n",
    "    :returns: tuple (train_df, val_df, test_df) as stratified splits\n",
    "    \"\"\"\n",
    "    assert df.shape[1] == 2, \"Dataframe should have 2 columns\"\n",
    "    assert 0 < n_folds and 0 < test_ratio < 1\n",
    "    path = df.iloc[:, 0]\n",
    "    target = df.iloc[:, -1]\n",
    "\n",
    "    # defining splits\n",
    "    path_train, path_test, target_train, target_test = train_test_split(path, target, test_size = test_ratio, random_state=42, stratify=target)\n",
    "    \n",
    "    train_df = pd.concat([path_train, target_train], axis=1)\n",
    "\n",
    "    # concatenating path and target data\n",
    "    folds = cv_split(train_df, n_folds)\n",
    "    test_df = pd.concat([path_test, target_test], axis=1)\n",
    "    \n",
    "    unique_classes = target.unique()\n",
    "    \n",
    "\n",
    "    return folds, test_df, unique_classes\n",
    "    \n",
    "def cv_split (df: pd.DataFrame, n_folds) -> list:\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    folds = []\n",
    "    for fold_index, fold in enumerate(skf.split(df, df.iloc[:, -1])):\n",
    "        foldtoapppend = df.iloc[fold[-1]]\n",
    "        folds.append(foldtoapppend)\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b3719",
   "metadata": {},
   "source": [
    "# Let's rename everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00a0c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_destination = '../Project/Data/rare_species 1/splits'\n",
    "origin_path = '../Project/Data/rare_species 1/images'\n",
    "\n",
    "def load_image (path: str,image_path: str) -> tf.Tensor:\n",
    "    print(f'file_path: {path + \"/\" + image_path}')\n",
    "    image = tf.io.read_file(path + '/' + image_path) # get the image \n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "def save_image (path: str, image_path: str, image: tf.Tensor) -> None:\n",
    "    image = tf.image.encode_jpeg(image)\n",
    "    tf.io.write_file(path + '/' + image_path, image)\n",
    "    \n",
    "def folder_rearrangement (df: pd.DataFrame, path_destination: str, origin_path: str,labels: list) -> None:\n",
    "    \"\"\"\n",
    "    Rearranges the images in the destination folder according to the DataFrame.\n",
    "\n",
    "    :param df (pd.DataFrame): DataFrame with [file_path, target_class] columns\n",
    "    :param path_destination (str): Path to the destination folder\n",
    "    :param origin_path (str): Path to the original folder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path_destination):\n",
    "        os.makedirs(path_destination)\n",
    "    \n",
    "    for label in labels:\n",
    "        if not os.path.exists(path_destination + '/' + label):\n",
    "            os.makedirs(path_destination + '/' + label)\n",
    "    for index, im in enumerate(df[df.iloc[:, 1] == label].iloc[:, 0]):\n",
    "        image = load_image(origin_path, im)\n",
    "        name_of_image = label + '_' + str(index).zfill(6) + '.jpg'\n",
    "        final_path_of_image = path_destination + '/' + label + '/' + name_of_image\n",
    "        save_image(path_destination, name_of_image, image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae03542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global definitions\n",
    "path = '../Project/Data/rare_species 1'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(path + '/metadata.csv')\n",
    "df.head()\n",
    "\n",
    "df = df[['file_path', 'phylum']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77ef3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_classes ['mollusca' 'chordata' 'arthropoda' 'echinodermata' 'cnidaria']\n",
      "file_path:                                                file_path  \\\n",
      "5181   chordata_elapidae/22401524_1055803_eol-full-si...   \n",
      "4459   chordata_callorhinchidae/29819269_46561158_eol...   \n",
      "5422   chordata_testudinidae/29620713_794282_eol-full...   \n",
      "6680   chordata_salamandridae/8936926_4357897_eol-ful...   \n",
      "9319   chordata_balaenidae/20140543_46559421_eol-full...   \n",
      "...                                                  ...   \n",
      "10064  chordata_hemiscylliidae/29599804_46559713_eol-...   \n",
      "6213   chordata_dasyatidae/29883670_46560871_eol-full...   \n",
      "9486   chordata_callitrichidae/28288244_323904_eol-fu...   \n",
      "8734   chordata_alopiidae/22310552_46559744_eol-full-...   \n",
      "10295  chordata_plethodontidae/20488656_336169_eol-fu...   \n",
      "\n",
      "                                                  phylum  \n",
      "5181   chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "4459   chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "5422   chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "6680   chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "9319   chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "...                                                  ...  \n",
      "10064  chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "6213   chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "9486   chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "8734   chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "10295  chordata/cnidaria_agariciidae/28307534_4527651...  \n",
      "\n",
      "[1918 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input filename tensor must be scalar, but had shape: [1918,2] [Op:ReadFile]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, fold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(list_of_folds):\n\u001b[1;32m      7\u001b[0m     path_fold \u001b[38;5;241m=\u001b[39m new_path_imgs \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/fold_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mfolder_rearrangement\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m, in \u001b[0;36mfolder_rearrangement\u001b[0;34m(df, path_destination, origin_path, labels)\u001b[0m\n\u001b[1;32m     27\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(path_destination \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m label)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, im \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df[df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m label]\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     name_of_image \u001b[38;5;241m=\u001b[39m label \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(index)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m6\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     31\u001b[0m     final_path_of_image \u001b[38;5;241m=\u001b[39m path_destination \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m label \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name_of_image\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(path, image_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image\u001b[39m (path: \u001b[38;5;28mstr\u001b[39m,image_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get the image \u001b[39;00m\n\u001b[1;32m      7\u001b[0m     image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_jpeg(image, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL_env/lib/python3.12/site-packages/tensorflow/python/ops/io_ops.py:134\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(filename, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reads the contents of file.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m  This operation returns a tensor with the entire contents of the input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    A tensor of dtype \"string\", with the file contents.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_io_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL_env/lib/python3.12/site-packages/tensorflow/python/ops/gen_io_ops.py:583\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_file_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m    586\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL_env/lib/python3.12/site-packages/tensorflow/python/ops/gen_io_ops.py:606\u001b[0m, in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    604\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [filename]\n\u001b[1;32m    605\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[1;32m    609\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[1;32m    610\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadFile\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL_env/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input filename tensor must be scalar, but had shape: [1918,2] [Op:ReadFile]"
     ]
    }
   ],
   "source": [
    "new_path_imgs = '/Users/leonardodicaterina/Desktop/NovaIMS/DL/data'\n",
    "df_reduced = df.iloc[:, :100]\n",
    "\n",
    "list_of_folds, test_df, unique_classes = create_split(df, n_folds=5, test_ratio=0.2)\n",
    "print(f\"unique_classes {unique_classes}\")\n",
    "for i, fold in enumerate(list_of_folds):\n",
    "    path_fold = new_path_imgs + '/fold_' + str(i)\n",
    "    folder_rearrangement( fold, path_fold, fold, unique_classes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
